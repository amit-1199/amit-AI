{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amit-1199/amit-AI/blob/main/text_preprocessing_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "561v5LUrDvwv"
      },
      "source": [
        "# Text Preprocessing Basics (Beginner Tutorial)\n",
        "\n",
        "This notebook shows **simple, easy-to-read examples** of common text preprocessing steps in Natural Language Processing (NLP).\n",
        "\n",
        "We will use a small sample text (from a public-style news sentence) and walk through these steps, always showing **before and after**:\n",
        "\n",
        "- Basic text cleaning (URLs, emojis, punctuation, line breaks, extra spaces, lowercasing)\n",
        "- Accent & diacritic normalization (e.g., cafÃ© â†’ cafe)\n",
        "- Sentence tokenisation\n",
        "- Word tokenisation\n",
        "- Subword tokenisation (using a pretrained tokenizer)\n",
        "- Stop word removal\n",
        "- Stemming\n",
        "- Lemmatization\n",
        "- POS (Part-of-Speech) tagging\n",
        "- NER (Named Entity Recognition)\n",
        "- **Identifying and removing outliers** in text datasets\n",
        "\n",
        "The code is meant for **beginners**, so we avoid clever tricks and keep everything very clear.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jupyter numpy pandas matplotlib nltk spacy transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eso129NTEG6S",
        "outputId": "1ad24bb4-306f-4b94-841c-b4028020e926"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jupyter\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.12/dist-packages (from jupyter) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.12/dist-packages (from jupyter) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (from jupyter) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from jupyter) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (from jupyter) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter)\n",
            "  Downloading jupyterlab-4.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter) (3.0.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter) (5.9.1)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.30 in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter) (2.19.2)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter) (0.28.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter)\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter) (2.14.0)\n",
            "Collecting jupyterlab-server<3,>=2.28.0 (from jupyterlab->jupyter)\n",
            "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (25.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (0.23.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (1.3.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (4.12.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-console->jupyter) (4.5.1)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.9.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook->jupyter) (25.1.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter)\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (4.25.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert->jupyter) (2.21.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.14)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.12/dist-packages (from terminado>=0.8.3->notebook->jupyter) (0.7.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.8)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (0.30.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (4.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.23)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (25.10.0)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.4.0)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading jupyterlab-4.5.0-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: json5, jedi, async-lru, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
            "Successfully installed async-lru-2.0.5 jedi-0.19.2 json5-0.12.1 jupyter-1.1.1 jupyter-lsp-2.3.0 jupyterlab-4.5.0 jupyterlab-server-2.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "d7QScQ6iEkrw",
        "outputId": "e6637db9-3f82-4ee5-e516-27addd2e0969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-581980377.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-581980377.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python -m spacy download en_core_web_sm\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZG274ZcDvww"
      },
      "outputs": [],
      "source": [
        "# Install and import libraries (run this cell once)\n",
        "\n",
        "# If you run this on your own machine, make sure you have installed the packages with:\n",
        "#  pip install jupyter numpy pandas matplotlib nltk spacy transformers\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Download small NLTK resources (only needs to be done once per machine)\n",
        "# These downloads may take a short time.\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Load spaCy small English model for POS and NER.\n",
        "# If you don't have it yet, run this once in a terminal or notebook cell:\n",
        "#   python -m spacy download en_core_web_sm\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load a simple pretrained tokenizer for subword tokenization\n",
        "subword_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "print(\"Libraries are ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idbjyxCDDvwx"
      },
      "outputs": [],
      "source": [
        "# Our sample text: 3 short paragraphs with emojis, URL, accents and messy spacing\n",
        "# below is the corpus\n",
        "sample_text = \"\"\"I loved visiting the cafÃ© in Paris ðŸ˜Š!!  The coffee was amazing,\n",
        " but the website https://example.com was a bit slow...\n",
        "\n",
        "Overall, it was a naÃ¯ve but fun choice ðŸ˜…!!!   We took many photos ðŸ“¸ðŸ“¸\n",
        " and shared them on social media.   Some people said it was too expensive.\n",
        "\n",
        "Later, we flew to SÃ£o Paulo to visit another cafe ðŸ˜€.   The streets were busy,\n",
        " the music was loud, and the desserts were delicious!!!   \"\"\"\n",
        "\n",
        "print(\"ORIGINAL TEXT (RAW):\\n\")\n",
        "print(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDjgbgnVDvwx"
      },
      "outputs": [],
      "source": [
        "# Helper: basic cleaning functions with BEFORE / AFTER prints\n",
        "\n",
        "def remove_urls(text: str) -> str:\n",
        "    pattern = r\"https?://\\S+\"\n",
        "    return re.sub(pattern, \"\", text)\n",
        "\n",
        "\n",
        "def normalize_accents_and_emojis(text: str) -> str:\n",
        "    \"\"\"Convert accents to plain letters and remove emojis/other symbols.\n",
        "\n",
        "    Steps:\n",
        "    1. Use Unicode normalization (NFKD) to split accents from letters.\n",
        "    2. Encode to ASCII and ignore characters we cannot represent (drops emojis).\n",
        "    3. Decode back to a normal string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize the text to remove accents and emojis (NFKD stands for \"Normalization Form\n",
        "    # KD\", which is a way to normalize text by decomposing characters into their base form and then recomposing them)\n",
        "    normalized = unicodedata.normalize(\"NFKD\", text)\n",
        "\n",
        "    # Encode to ASCII and ignore characters we cannot represent (drops emojis)\n",
        "    ascii_bytes = normalized.encode(\"ascii\", \"ignore\")\n",
        "    return ascii_bytes.decode(\"ascii\")\n",
        "\n",
        "\n",
        "def remove_linebreaks(text: str) -> str:\n",
        "    return text.replace(\"\\n\", \" \")\n",
        "\n",
        "\n",
        "def remove_extra_spaces(text: str) -> str:\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "\n",
        "def remove_punctuation(text: str) -> str:\n",
        "    # Keep only letters, numbers, and spaces\n",
        "    return re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", text)\n",
        "\n",
        "\n",
        "def to_lowercase(text: str) -> str:\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def show_before_after(step_name: str, before: str, after: str) -> None:\n",
        "    print(f\"===== {step_name} =====\")\n",
        "    print(\"BEFORE:\")\n",
        "    print(before)\n",
        "    print(\"\\nAFTER:\")\n",
        "    print(after)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_lpVzSjDvwx"
      },
      "outputs": [],
      "source": [
        "# 1. Basic cleaning pipeline with BEFORE / AFTER at each step\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"BASIC TEXT CLEANING PIPELINE\")\n",
        "print(\"==============================\\n\")\n",
        "\n",
        "# Start from the raw text with emojis, URL, accents, line breaks, and extra spaces\n",
        "step1 = remove_urls(sample_text)\n",
        "show_before_after(\"Remove URLs\", sample_text, step1)\n",
        "\n",
        "step2 = normalize_accents_and_emojis(step1)\n",
        "show_before_after(\"Normalize accents & remove emojis\", step1, step2)\n",
        "\n",
        "step3 = remove_linebreaks(step2)\n",
        "show_before_after(\"Remove line breaks\", step2, step3)\n",
        "\n",
        "step4 = remove_extra_spaces(step3)\n",
        "show_before_after(\"Remove extra spaces\", step3, step4)\n",
        "\n",
        "step5 = remove_punctuation(step4)\n",
        "show_before_after(\"Remove punctuation\", step4, step5)\n",
        "\n",
        "cleaned_text = to_lowercase(step5)\n",
        "show_before_after(\"Lowercase\", step5, cleaned_text)\n",
        "\n",
        "print(\"FINAL CLEANED TEXT (we will use this for tokenisation):\\n\")\n",
        "print(cleaned_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTLN_KvEDvwx"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 2. Sentence tokenisation (using the original raw text)\n",
        "\n",
        "print(\"===== Sentence tokenisation (NLTK) =====\\n\")\n",
        "print(\"Input text (raw):\\n\")\n",
        "print(sample_text)\n",
        "\n",
        "sentences = sent_tokenize(sample_text)\n",
        "\n",
        "print(\"\\nSentence tokens:\\n\")\n",
        "for i, sent in enumerate(sentences, start=1):\n",
        "    print(f\"Sentence {i}: {sent}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_locOhR3Dvwx"
      },
      "outputs": [],
      "source": [
        "# 3. Word tokenisation (using the cleaned text)\n",
        "\n",
        "print(\"===== Word tokenisation (NLTK) =====\\n\")\n",
        "print(\"Input text (cleaned):\\n\")\n",
        "print(cleaned_text)\n",
        "\n",
        "word_tokens = word_tokenize(cleaned_text)\n",
        "\n",
        "print(\"\\nFirst 50 word tokens:\\n\")\n",
        "print(word_tokens[:50])\n",
        "print(f\"\\nTotal number of word tokens: {len(word_tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Guhzj3b5Dvwx"
      },
      "outputs": [],
      "source": [
        "# 4. Stop word removal\n",
        "\n",
        "print(\"===== Stop word removal =====\\n\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "# stop_words = ['i', 'am', 'the', 'is', 'was']\n",
        "\n",
        "tokens_no_stop = [token for token in word_tokens if token not in stop_words]\n",
        "\n",
        "print(\"Original tokens (first 30):\\n\")\n",
        "print(word_tokens[:30])\n",
        "\n",
        "print(\"\\nAfter removing stop words (first 30):\\n\")\n",
        "print(tokens_no_stop[:30])\n",
        "\n",
        "print(f\"\\nTotal tokens before: {len(word_tokens)}\")\n",
        "print(f\"Total tokens after removing stop words: {len(tokens_no_stop)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CaN2OIKDvwx"
      },
      "outputs": [],
      "source": [
        "# 5. Stemming (PorterStemmer)\n",
        "\n",
        "print(\"===== Stemming (PorterStemmer) =====\\n\")\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens_no_stop]\n",
        "\n",
        "print(\"Original vs stemmed (first 30 tokens):\\n\")\n",
        "for original, stem in zip(tokens_no_stop[:30], stemmed_tokens[:30]):\n",
        "    print(f\"{original:15s} -> {stem}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQLgFis1Dvwx"
      },
      "outputs": [],
      "source": [
        "# 6. Lemmatization (WordNetLemmatizer)\n",
        "\n",
        "print(\"===== Lemmatization (WordNetLemmatizer) =====\\n\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens_no_stop]\n",
        "\n",
        "print(\"Original vs lemmatized (first 30 tokens):\\n\")\n",
        "for original, lemma in zip(tokens_no_stop[:30], lemmatized_tokens[:30]):\n",
        "    print(f\"{original:15s} -> {lemma}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-amHrMArDvwx"
      },
      "outputs": [],
      "source": [
        "# 7. POS tagging (Part-of-Speech) and NER (Named Entity Recognition) with spaCy\n",
        "\n",
        "print(\"===== POS tagging (spaCy) =====\\n\")\n",
        "\n",
        "# Use the original raw text so spaCy can see punctuation and capitalization\n",
        "pos_doc = nlp(sample_text)\n",
        "\n",
        "print(\"Token -> coarse POS tag (pos_) and fine-grained tag (tag_) (first 40 tokens):\\n\")\n",
        "for token in pos_doc[:40]:\n",
        "    print(f\"{token.text:15s} {token.pos_:10s} {token.tag_:10s}\")\n",
        "\n",
        "print(\"\\n===== Named Entity Recognition (spaCy) =====\\n\")\n",
        "\n",
        "print(\"Detected entities (text -> label):\\n\")\n",
        "for ent in pos_doc.ents:\n",
        "    print(f\"{ent.text:30s} -> {ent.label_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDE4HdoSDvwy"
      },
      "outputs": [],
      "source": [
        "# 8. Subword tokenisation (BERT-style tokenizer)\n",
        "\n",
        "print(\"===== Subword tokenisation (BERT tokenizer) =====\\n\")\n",
        "\n",
        "# We use the cleaned text here\n",
        "subword_tokens = subword_tokenizer.tokenize(cleaned_text)\n",
        "\n",
        "print(\"First 50 subword tokens:\\n\")\n",
        "print(subword_tokens[:50])\n",
        "\n",
        "input_ids = subword_tokenizer.encode(cleaned_text)\n",
        "print(\"\\nFirst 50 token IDs:\\n\")\n",
        "print(input_ids[:50])\n",
        "\n",
        "print(f\"\\nTotal number of subword tokens: {len(subword_tokens)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdJ8wzQzDvwy"
      },
      "source": [
        "---\n",
        "\n",
        "## 9. Identifying and Removing Outliers in Text Data\n",
        "\n",
        "**What are outliers in NLP?**\n",
        "\n",
        "Outliers in text data are documents that are very different from most other documents. Common examples:\n",
        "- **Very short texts**: Single words, empty strings, or just a few characters\n",
        "- **Very long texts**: Extremely long documents that are much longer than average\n",
        "- **Unusual content**: Texts with mostly special characters, numbers, or gibberish\n",
        "- **Duplicate texts**: Exact or near-duplicate documents\n",
        "\n",
        "**Why remove outliers?**\n",
        "\n",
        "Outliers can:\n",
        "- Skew your analysis and model training\n",
        "- Add noise to your dataset\n",
        "- Make it harder to find patterns in normal data\n",
        "\n",
        "**How to identify outliers?**\n",
        "\n",
        "We use simple statistics:\n",
        "- **Character count**: How many characters are in each document\n",
        "- **Word count**: How many words are in each document\n",
        "- **Average word length**: Average length of words in the document\n",
        "\n",
        "Then we find documents that are too far from the average (using thresholds or statistical methods).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brtBuEcoDvwy"
      },
      "outputs": [],
      "source": [
        "# Sample dataset: A collection of text documents (some normal, some outliers)\n",
        "\n",
        "# Normal texts (typical length)\n",
        "texts = [\n",
        "    \"The weather today is sunny and warm. I went for a walk in the park.\",\n",
        "    \"Machine learning is a fascinating field of study. It helps computers learn from data.\",\n",
        "    \"I love reading books in my free time. Fiction and non-fiction both interest me.\",\n",
        "    \"Cooking is one of my favorite hobbies. I enjoy trying new recipes on weekends.\",\n",
        "    \"The city has many beautiful parks and gardens. People love to visit them.\",\n",
        "    \"Technology has changed how we communicate. Social media connects people worldwide.\",\n",
        "    \"Music brings joy to many people. Different genres appeal to different tastes.\",\n",
        "    \"Exercise is important for maintaining good health. Regular activity keeps you fit.\",\n",
        "    \"Traveling allows you to experience different cultures. It broadens your perspective.\",\n",
        "    \"Education opens doors to many opportunities. Learning never stops throughout life.\"\n",
        "]\n",
        "\n",
        "# Outliers: very short, very long, or unusual content\n",
        "outlier_texts = [\n",
        "    \"Hi\",  # Too short (outlier)\n",
        "    \"\",  # Empty (outlier)\n",
        "    \"A\",  # Single character (outlier)\n",
        "    \"This is an extremely long text that goes on and on and on. \" * 50,  # Very long (outlier)\n",
        "    \"1234567890 !@#$%^&*() 9876543210\",  # Mostly numbers and symbols (outlier)\n",
        "    \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",  # Repetitive gibberish (outlier)\n",
        "]\n",
        "\n",
        "# Combine all texts\n",
        "all_texts = texts + outlier_texts\n",
        "\n",
        "print(\"===== Sample Dataset =====\\n\")\n",
        "print(f\"Total number of documents: {len(all_texts)}\\n\")\n",
        "print(\"First 3 normal texts:\")\n",
        "for i, text in enumerate(texts[:3], 1):\n",
        "    print(f\"{i}. {text[:60]}...\")\n",
        "print(\"\\nOutlier texts:\")\n",
        "for i, text in enumerate(outlier_texts, 1):\n",
        "    preview = text[:60] + \"...\" if len(text) > 60 else text\n",
        "    print(f\"{i}. {preview}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQaTaFJLDvwy"
      },
      "outputs": [],
      "source": [
        "# Calculate statistics for each document\n",
        "\n",
        "import statistics\n",
        "\n",
        "def calculate_stats(text: str) -> dict:\n",
        "    \"\"\"Calculate basic statistics for a text document.\"\"\"\n",
        "    char_count = len(text) # count the characters\n",
        "    words = text.split()\n",
        "    word_count = len(words) # Count the words\n",
        "    avg_word_length = statistics.mean([len(word) for word in words]) if words else 0 # average number of words in the sentence\n",
        "    return {\n",
        "        \"char_count\": char_count,\n",
        "        \"word_count\": word_count,\n",
        "        \"avg_word_length\": avg_word_length\n",
        "    }\n",
        "\n",
        "# Calculate stats for all documents\n",
        "all_stats = [calculate_stats(text) for text in all_texts]\n",
        "\n",
        "# Calculate average (mean) statistics\n",
        "avg_char_count = statistics.mean([s[\"char_count\"] for s in all_stats])\n",
        "avg_word_count = statistics.mean([s[\"word_count\"] for s in all_stats])\n",
        "avg_word_len = statistics.mean([s[\"avg_word_length\"] for s in all_stats])\n",
        "\n",
        "# Calculate standard deviation (how spread out the data is)\n",
        "std_char_count = statistics.stdev([s[\"char_count\"] for s in all_stats]) if len(all_stats) > 1 else 0\n",
        "std_word_count = statistics.stdev([s[\"word_count\"] for s in all_stats]) if len(all_stats) > 1 else 0\n",
        "\n",
        "print(\"===== Document Statistics =====\\n\")\n",
        "print(\"Statistics for each document:\\n\")\n",
        "for i, (text, stats) in enumerate(zip(all_texts, all_stats), 1):\n",
        "    preview = text[:50] + \"...\" if len(text) > 50 else text\n",
        "    print(f\"Doc {i:2d}: chars={stats['char_count']:4d}, words={stats['word_count']:3d}, \"\n",
        "          f\"avg_word_len={stats['avg_word_length']:.1f} | {preview}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"AVERAGE: chars={avg_char_count:.1f}, words={avg_word_count:.1f}, \"\n",
        "      f\"avg_word_len={avg_word_len:.1f}\")\n",
        "print(f\"STANDARD DEVIATION: chars={std_char_count:.1f}, words={std_word_count:.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz8gEIeGDvwy"
      },
      "outputs": [],
      "source": [
        "# Identify outliers using simple thresholds\n",
        "\n",
        "# Method 1: Using fixed thresholds (simple and easy to understand)\n",
        "MIN_CHARS = 10  # Documents with fewer than 10 characters are outliers\n",
        "MIN_WORDS = 3   # Documents with fewer than 3 words are outliers\n",
        "MAX_CHARS = 2000  # Documents with more than 2000 characters are outliers\n",
        "\n",
        "def is_outlier_fixed_threshold(text: str) -> bool:\n",
        "    \"\"\"Check if a text is an outlier using fixed thresholds.\"\"\"\n",
        "    stats = calculate_stats(text)\n",
        "    if stats[\"char_count\"] < MIN_CHARS or stats[\"char_count\"] > MAX_CHARS:\n",
        "        return True\n",
        "    if stats[\"word_count\"] < MIN_WORDS:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Method 2: Using statistical thresholds (mean Â± 2 standard deviations)\n",
        "# Documents that are more than 2 standard deviations away from the mean are outliers\n",
        "def is_outlier_statistical(text: str) -> bool:\n",
        "    \"\"\"Check if a text is an outlier using statistical method.\"\"\"\n",
        "    stats = calculate_stats(text)\n",
        "    # Check if character count is too far from average\n",
        "    if abs(stats[\"char_count\"] - avg_char_count) > 2 * std_char_count:\n",
        "        return True\n",
        "    # Check if word count is too far from average\n",
        "    if abs(stats[\"word_count\"] - avg_word_count) > 2 * std_word_count:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Identify outliers using both methods\n",
        "outliers_fixed = [i for i, text in enumerate(all_texts) if is_outlier_fixed_threshold(text)]\n",
        "outliers_statistical = [i for i, text in enumerate(all_texts) if is_outlier_statistical(text)]\n",
        "\n",
        "print(\"===== Outlier Detection =====\\n\")\n",
        "print(\"Method 1: Fixed Thresholds\")\n",
        "print(f\"  Min chars: {MIN_CHARS}, Min words: {MIN_WORDS}, Max chars: {MAX_CHARS}\")\n",
        "print(f\"  Outliers found: {outliers_fixed}\")\n",
        "print(\"\\nMethod 2: Statistical (mean Â± 2 standard deviations)\")\n",
        "print(f\"  Outliers found: {outliers_statistical}\\n\")\n",
        "\n",
        "print(\"Outlier documents identified:\\n\")\n",
        "for idx in outliers_fixed:\n",
        "    preview = all_texts[idx][:60] + \"...\" if len(all_texts[idx]) > 60 else all_texts[idx]\n",
        "    stats = all_stats[idx]\n",
        "    print(f\"  Doc {idx+1}: chars={stats['char_count']}, words={stats['word_count']} | {preview}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23-RQU1RDvwy"
      },
      "outputs": [],
      "source": [
        "# Remove outliers and show BEFORE / AFTER\n",
        "\n",
        "print(\"===== Removing Outliers =====\\n\")\n",
        "\n",
        "print(\"BEFORE (with outliers):\")\n",
        "print(f\"  Total documents: {len(all_texts)}\")\n",
        "print(f\"  Outliers: {len(outliers_fixed)} documents\")\n",
        "print(f\"  Normal documents: {len(all_texts) - len(outliers_fixed)} documents\\n\")\n",
        "\n",
        "# Remove outliers (keep only non-outliers)\n",
        "cleaned_texts = [text for i, text in enumerate(all_texts) if i not in outliers_fixed]\n",
        "cleaned_indices = [i for i in range(len(all_texts)) if i not in outliers_fixed]\n",
        "\n",
        "print(\"AFTER (outliers removed):\")\n",
        "print(f\"  Total documents: {len(cleaned_texts)}\")\n",
        "print(f\"  Removed: {len(outliers_fixed)} outlier documents\\n\")\n",
        "\n",
        "print(\"Removed outlier documents:\")\n",
        "for idx in outliers_fixed:\n",
        "    preview = all_texts[idx][:60] + \"...\" if len(all_texts[idx]) > 60 else all_texts[idx]\n",
        "    stats = all_stats[idx]\n",
        "    print(f\"  - Doc {idx+1}: chars={stats['char_count']}, words={stats['word_count']} | {preview}\")\n",
        "\n",
        "print(\"\\nKept normal documents (first 5):\")\n",
        "for i, idx in enumerate(cleaned_indices[:5], 1):\n",
        "    preview = cleaned_texts[i-1][:60] + \"...\" if len(cleaned_texts[i-1]) > 60 else cleaned_texts[i-1]\n",
        "    stats = all_stats[idx]\n",
        "    print(f\"  {i}. Doc {idx+1}: chars={stats['char_count']}, words={stats['word_count']} | {preview}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NajXd4dsDvwy"
      },
      "outputs": [],
      "source": [
        "# Summary: Statistics comparison BEFORE and AFTER removing outliers\n",
        "\n",
        "print(\"===== Summary: Statistics Comparison =====\\n\")\n",
        "\n",
        "# Calculate new statistics after removing outliers\n",
        "cleaned_stats = [calculate_stats(text) for text in cleaned_texts]\n",
        "cleaned_char_counts = [s[\"char_count\"] for s in cleaned_stats]\n",
        "cleaned_word_counts = [s[\"word_count\"] for s in cleaned_stats]\n",
        "\n",
        "print(\"BEFORE removing outliers:\")\n",
        "print(f\"  Average character count: {avg_char_count:.1f}\")\n",
        "print(f\"  Average word count: {avg_word_count:.1f}\")\n",
        "print(f\"  Character count range: {min([s['char_count'] for s in all_stats])} - {max([s['char_count'] for s in all_stats])}\")\n",
        "print(f\"  Word count range: {min([s['word_count'] for s in all_stats])} - {max([s['word_count'] for s in all_stats])}\")\n",
        "\n",
        "print(\"\\nAFTER removing outliers:\")\n",
        "print(f\"  Average character count: {statistics.mean(cleaned_char_counts):.1f}\")\n",
        "print(f\"  Average word count: {statistics.mean(cleaned_word_counts):.1f}\")\n",
        "print(f\"  Character count range: {min(cleaned_char_counts)} - {max(cleaned_char_counts)}\")\n",
        "print(f\"  Word count range: {min(cleaned_word_counts)} - {max(cleaned_word_counts)}\")\n",
        "\n",
        "print(\"\\nâœ… Outlier removal complete! The dataset is now cleaner and more consistent.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-3VjrHCDvwy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}